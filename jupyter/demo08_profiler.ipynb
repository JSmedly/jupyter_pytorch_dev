{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYTORCH PROFILER\n",
    "\n",
    "This recipe explains how to use PyTorch profiler and measure the time and memory consumption of the model’s operators.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "PyTorch includes a simple profiler API that is useful when user needs to determine the most expensive operators in the model.\n",
    "\n",
    "In this recipe, we will use a simple Resnet model to demonstrate how to use profiler to analyze model performance.\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Import all necessary libraries\n",
    "2. Instantiate a simple Resnet model\n",
    "3. Using profiler to analyze execution time\n",
    "4. Using profiler to analyze memory consumption\n",
    "5. Using tracing functionality\n",
    "6. Examining stack traces\n",
    "7. Visualizing data as a flamegraph\n",
    "8. Using profiler to analyze long-running jobs\n",
    "\n",
    "### 1. Import all necessary libraries\n",
    "In this recipe we will use ```torch```, ```torchvision.models``` and ```profiler``` modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate a simple Resnet model\n",
    "\n",
    "Let’s create an instance of a Resnet model and prepare an input for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18()\n",
    "inputs = torch.randn(5, 3, 224, 224)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using profiler to analyze execution time\n",
    "\n",
    "PyTorch profiler is enabled through the context manager and accepts a number of parameters, some of the most useful are:\n",
    "\n",
    "- ```activities``` - a list of activities to profile:\n",
    "    - ```ProfilerActivity.CPU``` - PyTorch operators, TorchScript functions and user-defined code labels (see ```record_function``` below);\n",
    "    - ```ProfilerActivity.CUDA``` - on-device CUDA kernels;\n",
    "- ```record_shapes``` - whether to record shapes of the operator inputs;\n",
    "- ```profile_memory``` - whether to report amount of memory consumed by model’s Tensors;\n",
    "- ```use_cuda``` - whether to measure execution time of CUDA kernels.\n",
    "\n",
    "Note: when using CUDA, profiler also shows the runtime CUDA events occuring on the host.\n",
    "\n",
    "Let’s see how we can use profiler to analyze the execution time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2022-03-27 06:07:52 409:409 init.cpp:129] function status failed with error CUPTI_ERROR_NOT_INITIALIZED (15)\n",
      "WARNING:2022-03-27 06:07:52 409:409 init.cpp:130] CUPTI initialization failed - CUDA profiler activities will be missing\n",
      "INFO:2022-03-27 06:07:52 409:409 init.cpp:132] If you see CUPTI_ERROR_INSUFFICIENT_PRIVILEGES, refer to https://developer.nvidia.com/nvidia-development-tools-solutions-err-nvgpuctrperm-cupti\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, profile_memory=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can use ```record_function``` context manager to label arbitrary code ranges with user provided names (```model_inference``` is used as a label in the example above).\n",
    "\n",
    "Profiler allows one to check which operators were called during the execution of a code range wrapped with a profiler context manager. If multiple profiler ranges are active at the same time (e.g. in parallel PyTorch threads), each profiling context manager tracks only the operators of its corresponding range. Profiler also automatically profiles the async tasks launched with ```torch.jit._fork``` and (in case of a backward pass) the backward pass operators launched with ```backward()``` call.\n",
    "\n",
    "Let’s print out the stats for the execution above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  model_inference         5.20%      10.276ms        99.97%     197.365ms     197.365ms          -4 b    -106.30 Mb             1  \n",
      "                     aten::conv2d         0.15%     296.000us        67.81%     133.883ms       6.694ms      47.37 Mb           0 b            20  \n",
      "                aten::convolution         0.40%     784.000us        67.66%     133.587ms       6.679ms      47.37 Mb           0 b            20  \n",
      "               aten::_convolution         0.28%     547.000us        67.27%     132.803ms       6.640ms      47.37 Mb           0 b            20  \n",
      "         aten::mkldnn_convolution        66.52%     131.340ms        66.99%     132.256ms       6.613ms      47.37 Mb           0 b            20  \n",
      "                 aten::batch_norm         0.10%     205.000us        19.20%      37.905ms       1.895ms      47.41 Mb           0 b            20  \n",
      "     aten::_batch_norm_impl_index         0.19%     366.000us        19.10%      37.700ms       1.885ms      47.41 Mb           0 b            20  \n",
      "          aten::native_batch_norm        12.75%      25.171ms        18.87%      37.260ms       1.863ms      47.41 Mb     -75.00 Kb            20  \n",
      "                       aten::mean         0.52%       1.033ms         5.29%      10.452ms     497.714us      28.75 Kb      28.75 Kb            21  \n",
      "                        aten::sum         3.56%       7.020ms         3.65%       7.205ms     343.095us           0 b           0 b            21  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 197.430ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that, as expected, most of the time is spent in convolution (and specifically in ```mkldnn_convolution``` for PyTorch compiled with MKL-DNN support). Note the difference between self cpu time and cpu time - operators can call other operators, self cpu time exludes time spent in children operator calls, while total cpu time includes it. You can choose to sort by the self cpu time by passing ```sort_by=\"self_cpu_time_total\"``` into the ```table``` call.\n",
    "\n",
    "To get a finer granularity of results and include operator input shapes, pass ```group_by_input_shape=True``` (note: this requires running the profiler with ```record_shapes=True```):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls                                                                      Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                  model_inference         5.20%      10.276ms        99.97%     197.365ms     197.365ms          -4 b    -106.30 Mb             1                                                                                []  \n",
      "                     aten::conv2d         0.04%      81.000us        15.33%      30.262ms       7.566ms      15.31 Mb           0 b             4                             [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []]  \n",
      "                aten::convolution         0.10%     192.000us        15.29%      30.181ms       7.545ms      15.31 Mb           0 b             4                     [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], []]  \n",
      "               aten::_convolution         0.07%     138.000us        15.19%      29.989ms       7.497ms      15.31 Mb           0 b             4     [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], [], [], [], [], []]  \n",
      "         aten::mkldnn_convolution        14.92%      29.453ms        15.12%      29.851ms       7.463ms      15.31 Mb           0 b             4                             [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []]  \n",
      "                     aten::conv2d         0.02%      38.000us        15.03%      29.680ms       9.893ms       1.44 Mb           0 b             3                            [[5, 512, 7, 7], [512, 512, 3, 3], [], [], [], [], []]  \n",
      "                aten::convolution         0.05%     107.000us        15.01%      29.642ms       9.881ms       1.44 Mb           0 b             3                    [[5, 512, 7, 7], [512, 512, 3, 3], [], [], [], [], [], [], []]  \n",
      "               aten::_convolution         0.05%      96.000us        14.96%      29.535ms       9.845ms       1.44 Mb           0 b             3    [[5, 512, 7, 7], [512, 512, 3, 3], [], [], [], [], [], [], [], [], [], [], []]  \n",
      "         aten::mkldnn_convolution        14.86%      29.329ms        14.91%      29.439ms       9.813ms       1.44 Mb           0 b             3                            [[5, 512, 7, 7], [512, 512, 3, 3], [], [], [], [], []]  \n",
      "                     aten::conv2d         0.01%      23.000us        11.54%      22.786ms      22.786ms      15.31 Mb           0 b             1                             [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], []]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "Self CPU time total: 197.430ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the occurence of ```aten::convolution``` twice with different input shapes.\n",
    "\n",
    "Profiler can also be used to analyze performance of models executed on GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18().cuda()\n",
    "inputs = torch.randn(5, 3, 224, 224).cuda()\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CUDA],record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYTORCH PROFILER\n",
    "\n",
    "This recipe explains how to use PyTorch profiler and measure the time and memory consumption of the model’s operators.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "PyTorch includes a simple profiler API that is useful when user needs to determine the most expensive operators in the model.\n",
    "\n",
    "In this recipe, we will use a simple Resnet model to demonstrate how to use profiler to analyze model performance.\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Import all necessary libraries\n",
    "2. Instantiate a simple Resnet model\n",
    "3. Using profiler to analyze execution time\n",
    "4. Using profiler to analyze memory consumption\n",
    "5. Using tracing functionality\n",
    "6. Examining stack traces\n",
    "7. Visualizing data as a flamegraph\n",
    "8. Using profiler to analyze long-running jobs\n",
    "\n",
    "### 1. Import all necessary libraries\n",
    "In this recipe we will use ```torch```, ```torchvision.models``` and ```profiler``` modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate a simple Resnet model\n",
    "\n",
    "Let’s create an instance of a Resnet model and prepare an input for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18()\n",
    "inputs = torch.randn(5, 3, 224, 224)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using profiler to analyze execution time\n",
    "\n",
    "PyTorch profiler is enabled through the context manager and accepts a number of parameters, some of the most useful are:\n",
    "\n",
    "- ```activities``` - a list of activities to profile:\n",
    "    - ```ProfilerActivity.CPU``` - PyTorch operators, TorchScript functions and user-defined code labels (see ```record_function``` below);\n",
    "    - ```ProfilerActivity.CUDA``` - on-device CUDA kernels;\n",
    "- ```record_shapes``` - whether to record shapes of the operator inputs;\n",
    "- ```profile_memory``` - whether to report amount of memory consumed by model’s Tensors;\n",
    "- ```use_cuda``` - whether to measure execution time of CUDA kernels.\n",
    "\n",
    "Note: when using CUDA, profiler also shows the runtime CUDA events occuring on the host.\n",
    "\n",
    "Let’s see how we can use profiler to analyze the execution time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, profile_memory=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can use ```record_function``` context manager to label arbitrary code ranges with user provided names (```model_inference``` is used as a label in the example above).\n",
    "\n",
    "Profiler allows one to check which operators were called during the execution of a code range wrapped with a profiler context manager. If multiple profiler ranges are active at the same time (e.g. in parallel PyTorch threads), each profiling context manager tracks only the operators of its corresponding range. Profiler also automatically profiles the async tasks launched with ```torch.jit._fork``` and (in case of a backward pass) the backward pass operators launched with ```backward()``` call.\n",
    "\n",
    "Let’s print out the stats for the execution above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  model_inference         1.89%       7.389ms        99.41%     388.669ms     388.669ms          -4 b    -106.30 Mb             1  \n",
      "                     aten::conv2d         0.63%       2.474ms        82.83%     323.844ms      16.192ms      47.37 Mb           0 b            20  \n",
      "                aten::convolution         0.13%     515.000us        82.20%     321.370ms      16.069ms      47.37 Mb           0 b            20  \n",
      "               aten::_convolution         0.17%     652.000us        82.07%     320.855ms      16.043ms      47.37 Mb           0 b            20  \n",
      "         aten::mkldnn_convolution        81.75%     319.613ms        81.90%     320.203ms      16.010ms      47.37 Mb           0 b            20  \n",
      "                 aten::batch_norm         0.11%     446.000us        10.51%      41.103ms       2.055ms      47.41 Mb           0 b            20  \n",
      "     aten::_batch_norm_impl_index         0.24%     944.000us        10.40%      40.657ms       2.033ms      47.41 Mb           0 b            20  \n",
      "          aten::native_batch_norm         9.05%      35.399ms        10.15%      39.683ms       1.984ms      47.41 Mb     -75.00 Kb            20  \n",
      "                 aten::max_pool2d         0.01%      57.000us         2.64%      10.311ms      10.311ms      11.48 Mb           0 b             1  \n",
      "    aten::max_pool2d_with_indices         2.62%      10.254ms         2.62%      10.254ms      10.254ms      11.48 Mb      11.48 Mb             1  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 390.967ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that, as expected, most of the time is spent in convolution (and specifically in ```mkldnn_convolution``` for PyTorch compiled with MKL-DNN support). Note the difference between self cpu time and cpu time - operators can call other operators, self cpu time exludes time spent in children operator calls, while total cpu time includes it. You can choose to sort by the self cpu time by passing ```sort_by=\"self_cpu_time_total\"``` into the ```table``` call.\n",
    "\n",
    "To get a finer granularity of results and include operator input shapes, pass ```group_by_input_shape=True``` (note: this requires running the profiler with ```record_shapes=True```):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls                                                                      Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                  model_inference         1.89%       7.389ms        99.41%     388.669ms     388.669ms          -4 b    -106.30 Mb             1                                                                                []  \n",
      "                     aten::conv2d         0.58%       2.270ms        34.79%     136.002ms     136.002ms      15.31 Mb           0 b             1                             [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], []]  \n",
      "                aten::convolution         0.03%     131.000us        34.21%     133.732ms     133.732ms      15.31 Mb           0 b             1                     [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], [], [], []]  \n",
      "               aten::_convolution         0.09%     364.000us        34.17%     133.601ms     133.601ms      15.31 Mb           0 b             1     [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], [], [], [], [], [], [], []]  \n",
      "         aten::mkldnn_convolution        34.06%     133.174ms        34.08%     133.237ms     133.237ms      15.31 Mb           0 b             1                             [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], []]  \n",
      "                     aten::conv2d         0.02%      62.000us        15.96%      62.394ms      15.598ms      15.31 Mb           0 b             4                             [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []]  \n",
      "                aten::convolution         0.03%     107.000us        15.94%      62.332ms      15.583ms      15.31 Mb           0 b             4                     [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], []]  \n",
      "               aten::_convolution         0.02%      77.000us        15.92%      62.225ms      15.556ms      15.31 Mb           0 b             4     [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], [], [], [], [], []]  \n",
      "         aten::mkldnn_convolution        15.82%      61.833ms        15.90%      62.148ms      15.537ms      15.31 Mb           0 b             4                             [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []]  \n",
      "                     aten::conv2d         0.01%      28.000us         9.48%      37.046ms      12.349ms       1.44 Mb           0 b             3                            [[5, 512, 7, 7], [512, 512, 3, 3], [], [], [], [], []]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "Self CPU time total: 390.967ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the occurence of ```aten::convolution``` twice with different input shapes.\n",
    "\n",
    "Profiler can also be used to analyze performance of models executed on GPUs: (Note: the first use of CUDA profiling may bring an extra overhead.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18().cuda()\n",
    "inputs = torch.randn(5, 3, 224, 224).cuda()\n",
    "\n",
    "with profile(activities=[\n",
    "        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the occurence of on-device kernels in the output (e.g. ```sgemm_32x32x32_NN```)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
